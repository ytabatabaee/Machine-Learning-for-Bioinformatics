{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "efrE1Lc0Otrq"
   },
   "source": [
    "# **Machine Learning in Bioinformatics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GtzvENmYOtxi"
   },
   "source": [
    "**Homework 3:**<br/>\n",
    "!!! If you don't fill these fields, your homework does not count !!!<br/>\n",
    "first name and last name : Yasamin Tabatabaee<br/>\n",
    "student number : 95104866"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KG3vb72VOt4G"
   },
   "source": [
    "You can run cells by hitting `Shift` + `Enter` or `ctrl` + `Enter`.<br/>\n",
    "We highly recommend you to read each line of code carefully and try to \n",
    "understand what it exactly does.<br/>\n",
    "Just alter the parts that is between green comments and specified for you. <br/>\n",
    "Please do not change other parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v0dotHjRO5x_"
   },
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_5C6lgUtO-bg"
   },
   "source": [
    "\n",
    "### about the Data:<br/>\n",
    "The purpose of this project is to classify tumors into malignant or benign. The following dataset is constructed based on images of tumors. Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass.\n",
    "For more details about the features of this dataset you can visit this link:\n",
    "https://scikit-learn.org/stable/datasets/index.html#breast-cancer-dataset<br/>\n",
    "This dataset contains 30 features and 1 label called target.\n",
    "The original dataset labels are 0 and 1 and in the following code boxes we change it to -1 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-7_g8ApcO7tm"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                 0.07871  ...          17.33           184.60      2019.0   \n",
       "1                 0.05667  ...          23.41           158.80      1956.0   \n",
       "2                 0.05999  ...          25.53           152.50      1709.0   \n",
       "3                 0.09744  ...          26.50            98.87       567.7   \n",
       "4                 0.05883  ...          16.67           152.20      1575.0   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  target  \n",
       "0          0.4601                  0.11890     0.0  \n",
       "1          0.2750                  0.08902     0.0  \n",
       "2          0.3613                  0.08758     0.0  \n",
       "3          0.6638                  0.17300     0.0  \n",
       "4          0.2364                  0.07678     0.0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer = load_breast_cancer()  ## change if the data set changed\n",
    "df = pd.DataFrame(np.c_[cancer[\"data\"], cancer[\"target\"]], columns = np.append(cancer[\"feature_names\"],[\"target\"]))\n",
    "features = ['mean radius', 'mean texture', 'mean perimeter', 'mean area', 'mean smoothness', 'mean compactness', 'mean concavity', 'mean concave points', 'mean symmetry', 'mean fractal dimension', 'radius error', 'texture error', 'perimeter error', 'area error', 'smoothness error', 'compactness error', 'concavity error', 'concave points error', 'symmetry error', 'fractal dimension error', 'worst radius', 'worst texture', 'worst perimeter', 'worst area', 'worst smoothness', 'worst compactness', 'worst concavity', 'worst concave points', 'worst symmetry', 'worst fractal dimension']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jmmK95OVPDyg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69.94727592267135 %\n",
      "69.94727592267135 %\n",
      "20.035149384885763 %\n",
      "20.035149384885763 %\n",
      "10.017574692442881 %\n",
      "10.017574692442881 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "cancer.target = np.where(cancer.target==0, -1, cancer.target)\n",
    "X_train ,X_test ,X_val ,y_train ,y_test ,y_val = None ,None ,None ,None ,None ,None\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# 1- Normalize tha data.                                                       #\n",
    "# 2- using train_test_split package, split your data into 3 numpy array        #\n",
    "# called X_train, X_test, and X_val and also split the corresponding labels as #\n",
    "# y_train, y_test, and y_val. After spliting, the ratio of your data should be # \n",
    "# approximately like this:                                                     #\n",
    "#  Train : 70%     test : 20%       validation : 10%                           #\n",
    "################################################################################\n",
    "\n",
    "cancer.data = preprocessing.normalize(cancer.data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size = 0.2)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.125)\n",
    "\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             #\n",
    "################################################################################\n",
    "print((X_train.shape[0]/cancer.data.shape[0]) * 100, \"%\")\n",
    "print((y_train.shape[0]/cancer.data.shape[0]) * 100, \"%\")\n",
    "print((X_test.shape[0]/cancer.data.shape[0]) * 100, \"%\")\n",
    "print((y_test.shape[0]/cancer.data.shape[0]) * 100, \"%\")\n",
    "print((X_val.shape[0]/cancer.data.shape[0]) * 100, \"%\")\n",
    "print((y_val.shape[0]/cancer.data.shape[0]) * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EZ3-Fm4uPIdf"
   },
   "source": [
    "# Ensemble Methods\n",
    "\n",
    "## Problem 1. Bagging (15 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy_num(cls_nums, cls_scores):\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(cls_nums, cls_scores)\n",
    "    plt.xlabel(\"Number of Classifiers\")\n",
    "    plt.ylabel(\"Validation Accuracy\")\n",
    "    plt.title(\"Accuracy - Num of Classifiers\")\n",
    "    plt.show()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZzSuFIANPPRh"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-d4a5d3bec29b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m                           \u001b[0mn_estimators\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_cls\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m                           random_state = seed)\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m     \u001b[0mcls_scores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnum_cls\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls_scores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnum_cls\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\y.tabatabaee\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\bagging.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    239\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m         \"\"\"\n\u001b[1;32m--> 241\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_parallel_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\y.tabatabaee\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\bagging.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, max_samples, max_depth, sample_weight)\u001b[0m\n\u001b[0;32m    377\u001b[0m                 \u001b[0mtotal_n_estimators\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m                 verbose=self.verbose)\n\u001b[1;32m--> 379\u001b[1;33m             for i in range(n_jobs))\n\u001b[0m\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m         \u001b[1;31m# Reduce\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\y.tabatabaee\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1001\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1002\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1003\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1004\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\y.tabatabaee\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    832\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    833\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 834\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    835\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    836\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\y.tabatabaee\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    751\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    752\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 753\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    754\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    755\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\y.tabatabaee\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\y.tabatabaee\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    580\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 582\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\y.tabatabaee\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 256\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\y.tabatabaee\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 256\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\y.tabatabaee\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\bagging.py\u001b[0m in \u001b[0;36m_parallel_build_estimators\u001b[1;34m(n_estimators, ensemble, X, y, sample_weight, seeds, total_n_estimators, verbose)\u001b[0m\n\u001b[0;32m    106\u001b[0m                 \u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnot_indices_mask\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\y.tabatabaee\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    814\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    815\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 816\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m    817\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\y.tabatabaee\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    378\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection \n",
    "from sklearn.ensemble import BaggingClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn\n",
    "\n",
    "################################################################################\n",
    "# TODO : initialize the base classifier. You can choose one of the classifiers #\n",
    "# you have learned in this course.(SVM/Decision tree)                          #\n",
    "# IMPORTANT: if you are using SVM as base classifier don't forget to add column#\n",
    "# of '1' s for bias and be careful to use the right datset in next parts.      #\n",
    "################################################################################\n",
    "\n",
    "base_cls = DecisionTreeClassifier()\n",
    "  \n",
    "##################################################################################\n",
    "# TODO: Number of classifiers is a hyperparameter. Choose it by using validation #\n",
    "# data to have the best accuracy                                                 #\n",
    "# For different number of classifiers, train the model with training data and    #\n",
    "# compute accuracy for validation data. Plot accuracy-number of classifiers plot.#\n",
    "##################################################################################\n",
    "seed = 123456\n",
    "max_cls = 100\n",
    "\n",
    "cls_scores = np.zeros(max_cls)\n",
    "best_model = None\n",
    "best_score = 0.0\n",
    "best_num = 0\n",
    "\n",
    "for num_cls in range(1, max_cls + 1):\n",
    "    model = BaggingClassifier(base_estimator = base_cls, \n",
    "                          n_estimators = num_cls, \n",
    "                          random_state = seed)\n",
    "    model.fit(X_train, y_train)\n",
    "    cls_scores[num_cls - 1] = model.score(X_val, y_val)\n",
    "    if cls_scores[num_cls - 1] > best_score:\n",
    "        best_num = num_cls\n",
    "        best_model = model\n",
    "        best_score = cls_scores[num_cls - 1]\n",
    "    \n",
    "plot_accuracy_num(range(1, max_cls + 1), cls_scores) \n",
    "print(\"Best validation accuracy is \", best_score * 100, \" for \", best_num, \" number of classifiers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# compute and report the accuracy for test data.                               #\n",
    "################################################################################\n",
    "y_preds = best_model.predict(X_test)\n",
    "print(\"Accuracy for test data = \", accuracy_score(y_test, y_preds) * 100)                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ELyA0acXgK2w"
   },
   "source": [
    "## Problem 2. Random Forest(25 points)</br>\n",
    "In this part, you should write your own code to classify the data, using random forest from sklearn package in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JE4eKUybgQUb"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#################################################################################\n",
    "# TODO:use the validation data to determine hyperparameters(number and depth of #\n",
    "# trees) for the best accuracy                                                  # \n",
    "#################################################################################\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start = 1, stop = 1000, num = 5)]\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 5)]\n",
    "params = {'n_estimators': n_estimators, 'max_depth': max_depth}\n",
    "\n",
    "random_forest = RandomForestClassifier()\n",
    "\n",
    "rf_random = GridSearchCV(estimator = random_forest, param_grid = params, cv = 5)\n",
    "rf_random.fit(X_train, y_train)\n",
    "print(model.score(X_val, y_val))\n",
    "\n",
    "#max_trees = 10\n",
    "#max_depth = 10\n",
    "\n",
    "\n",
    "#######################################################################################\n",
    "#TODO:report accuracy, presition,recall and confusion matrix for train and test data  #\n",
    "#######################################################################################\n",
    "\n",
    "y_preds = best_model.predict(X_test)\n",
    "print(\"Accuracy for test data = \", accuracy_score(y_test, y_preds) * 100)\n",
    "print(\"Confusion Matrix = \\n\", confusion_matrix(y_test, y_preds))\n",
    "print(\"Classification Report = \\n\", classification_report(y_test, y_preds))\n",
    "print(\"Avg Precision = \", classification_report(y_test, y_preds, output_dict=True)['weighted avg']['precision'])\n",
    "print(\"Avg Recall = \", classification_report(y_test, y_preds, output_dict=True)['weighted avg']['recall']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pSr7G0fdgmyf"
   },
   "source": [
    "### Question:\n",
    "Explain how you did choose the hyperparameters.</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "etxNZ36Ugnp7"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fxpbM42MPg6m"
   },
   "source": [
    "## Problem 3. Boosting : AdaBoost (35 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rUodQdBvPrKQ"
   },
   "source": [
    "In this part you should implement adaptive boosting algorithm. </br>\n",
    "<picture>\n",
    "  <img src=\"http://uupload.ir/files/b919_adaboost.png\" alt=\"Adaboost\" width=\"600\" height=\"300\">\n",
    "</picture>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r9TL5FGqRIoL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training data accuracy is: 92.32627276105538 %\n",
      "The test data accuracy is: 88.57707509881422 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "X_train ,X_test ,y_train ,y_test = None ,None ,None ,None\n",
    "###################################################################\n",
    "# TODO: use 80% of normalized data as train and 20% as test data. #\n",
    "###################################################################\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size = 0.2)\n",
    "\n",
    "######################################################################\n",
    "#TODO : define a weak decision tree.                                 #\n",
    "# initialize these parameters: criterion=\"entropy\" and max_depth = 1 #\n",
    "######################################################################\n",
    "\n",
    "Tree_model = DecisionTreeClassifier(criterion = \"entropy\", max_depth = 1)\n",
    "\n",
    "#############################################################################################\n",
    "#TODO : report accuracy of your weak model on train and test data by using cross validation #\n",
    "#############################################################################################\n",
    "\n",
    "train_accuracy = np.mean(cross_validate(Tree_model, X_train, y_train, cv = 5)['test_score']) \n",
    "print('The training data accuracy is:' , train_accuracy * 100 , '%')\n",
    "\n",
    "test_accuracy = np.mean(cross_validate(Tree_model, X_test, y_test, cv = 5)['test_score']) \n",
    "print('The test data accuracy is:' , test_accuracy * 100 , '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "elsmjgbNRSdH"
   },
   "outputs": [],
   "source": [
    "class AdaBoost:\n",
    "    \n",
    "    def __init__(self,train_data_X,train_data_y,tree_num,test_data_X,test_data_y):\n",
    "        self.train_data_X = train_data_X\n",
    "        self.train_data_y = train_data_y\n",
    "        self.tree_num = tree_num\n",
    "        self.test_data_X = test_data_X\n",
    "        self.test_data_y = test_data_y\n",
    "        self.alphas = None\n",
    "        self.models = None\n",
    "        self.accuracy = []\n",
    "        self.predictions = None\n",
    "        \n",
    "    def fit(self):\n",
    "        Evaluation = pd.DataFrame(self.train_data_y.copy())\n",
    "        Evaluation.columns = ['target']\n",
    "        ## TODO:Set the initial weights w = 1/N\n",
    "        Evaluation['weights'] = np.ones(self.train_data_X.shape[0]) / self.train_data_X.shape[0]\n",
    "        \n",
    "        alphas = [] #list of alphas \n",
    "        models = [] # list of trained models\n",
    "        for t in range(self.tree_num):\n",
    "            ## TODO: create a weak decisiontree classifier\n",
    "            Tree_model = DecisionTreeClassifier(criterion = \"entropy\", max_depth = 1)\n",
    "            ## TODO: fit the model with train data. set the sample_weight parameter to the 'weights' columns in Evaluation dataframe\n",
    "            model = Tree_model.fit(self.train_data_X, self.train_data_y, sample_weight = Evaluation['weights'].to_numpy()) \n",
    "            \n",
    "            models.append(model)\n",
    "            predictions = model.predict(self.train_data_X)\n",
    "            score = model.score(self.train_data_X, self.train_data_y)\n",
    "\n",
    "            ## Add this columns to the Evaluation DataFrame\n",
    "            Evaluation['predictions'] = predictions\n",
    "            ## TODO: In each row if the prediction and the target are equal,this column must be '1' and '0' O.W. \n",
    "            Evaluation['evaluation'] = (predictions == self.train_data_y) * 1\n",
    "            ## TODO: In each row if the tha data is missclassified, this column must be 1.\n",
    "            Evaluation['misclassified'] = 1 - Evaluation['evaluation']\n",
    "\n",
    "            ## TODO: Calculate the misclassification rate and accuracy and then use them to calculate error\n",
    "            accuracy = sum(Evaluation['evaluation'].to_numpy()) / self.train_data_X.shape[0] \n",
    "            misclassification = sum(Evaluation['misclassified'].to_numpy()) / self.train_data_X.shape[0] \n",
    "            err = sum(Evaluation['weights'].to_numpy() * Evaluation['misclassified'].to_numpy())\n",
    "            \n",
    "            ## TODO: Calculate the alpha values from the adaboost algorithm\n",
    "            alpha = 0.5 * np.log((1 - err) / err)\n",
    "            alphas.append(alpha)\n",
    "            ## TODO: update the weights\n",
    "            Evaluation['weights'] = Evaluation['weights'].to_numpy() * np.exp(-alpha * self.train_data_y * Evaluation['predictions'].to_numpy())\n",
    "            Evaluation['weights'] = Evaluation['weights'].to_numpy() / np.sum(Evaluation['weights'].to_numpy())\n",
    "            \n",
    "        self.alphas = alphas\n",
    "        self.models = models\n",
    "        \n",
    "        \n",
    "    def predict(self):\n",
    "        \n",
    "        accuracy = []\n",
    "        predictions = []\n",
    "        #####################################################################################\n",
    "        #TODO:                                                                              #\n",
    "        # 1- predict target for test data and append each prediction to the predictions list#\n",
    "        # 2- Create a list of accuracies which can be used to plot the accuracy against the #\n",
    "        # number of base learners used for the model                                        #\n",
    "        #####################################################################################\n",
    "        for alpha, model in zip(self.alphas, self.models):\n",
    "            prediction = model.predict(self.test_data_X)\n",
    "            predictions.append(prediction)\n",
    "            y_preds = np.sign(np.sum(np.array(predictions),axis=0))\n",
    "            self.accuracy.append(accuracy_score(self.test_data_y, y_preds) * 100)\n",
    "\n",
    "            \n",
    "        self.predictions = np.sign(np.sum(np.array(predictions),axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-by9WfOXRVQG"
   },
   "outputs": [],
   "source": [
    "# Accuracy - number of base learners plot for training data\n",
    "\n",
    "number_of_base_learners = 100\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax0 = fig.add_subplot(111)\n",
    "\n",
    "\n",
    "for i in range(number_of_base_learners):\n",
    "    model = AdaBoost(X_train,y_train,number_of_base_learners,X_train,y_train)\n",
    "    model.fit()\n",
    "    model.predict()\n",
    "\n",
    "ax0.plot(range(len(model.accuracy)),model.accuracy,'-b')\n",
    "ax0.set_xlabel('# models used for Boosting ')\n",
    "ax0.set_ylabel('training accuracy')\n",
    "print('With a number of ',number_of_base_learners,'base models we receive an accuracy of ',model.accuracy[-1],'%')    \n",
    "                 \n",
    "plt.show()   \n",
    "\n",
    "#################################################################### \n",
    "# TODO: Plot Accuracy - number of base learners plot for test data #\n",
    "#################################################################### \n",
    "\n",
    "number_of_base_learners = 100\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax0 = fig.add_subplot(111)\n",
    "\n",
    "\n",
    "for i in range(number_of_base_learners):\n",
    "    model = AdaBoost(X_train,y_train,number_of_base_learners,X_test,y_test)\n",
    "    model.fit()\n",
    "    model.predict()\n",
    "\n",
    "ax0.plot(range(len(model.accuracy)),model.accuracy,'-b')\n",
    "ax0.set_xlabel('# models used for Boosting ')\n",
    "ax0.set_ylabel('test accuracy')\n",
    "print('With a number of ',number_of_base_learners,'base models we receive an accuracy of ',model.accuracy[-1],'%')    \n",
    "                 \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2olecphWRi4L"
   },
   "source": [
    "# Feature Selction </br>\n",
    "\n",
    "## problem4. Filtering : correlation coefficient (25 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oe0ynxveRmXS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Correlation coefficient between features and target : \n",
      " mean radius                0.730029\n",
      "mean texture               0.415185\n",
      "mean perimeter             0.742636\n",
      "mean area                  0.708984\n",
      "mean smoothness            0.358560\n",
      "mean compactness           0.596534\n",
      "mean concavity             0.696360\n",
      "mean concave points        0.776614\n",
      "mean symmetry              0.330499\n",
      "mean fractal dimension     0.012838\n",
      "radius error               0.567134\n",
      "texture error              0.008303\n",
      "perimeter error            0.556141\n",
      "area error                 0.548236\n",
      "smoothness error           0.067016\n",
      "compactness error          0.292999\n",
      "concavity error            0.253730\n",
      "concave points error       0.408042\n",
      "symmetry error             0.006522\n",
      "fractal dimension error    0.077972\n",
      "worst radius               0.776454\n",
      "worst texture              0.456903\n",
      "worst perimeter            0.782914\n",
      "worst area                 0.733825\n",
      "worst smoothness           0.421465\n",
      "worst compactness          0.590998\n",
      "worst concavity            0.659610\n",
      "worst concave points       0.793566\n",
      "worst symmetry             0.416294\n",
      "worst fractal dimension    0.323872\n",
      "Name: target, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "################################################################################# \n",
    "# TODO:                                                                         #\n",
    "# use 80% of normalized data as train and 20% as test data.(just use the data   # \n",
    "# from last part)                                                               #\n",
    "# 1- compute the correlation coefficient between each feature and target.       #\n",
    "\n",
    "corr  = np.abs(df[df.columns[:]].corr()['target'][:df.shape[1] - 1])\n",
    "print(\"* Correlation coefficient between features and target : \\n\", corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Features with correlation bigger that 0.5 : \n",
      " mean radius             0.730029\n",
      "mean perimeter          0.742636\n",
      "mean area               0.708984\n",
      "mean compactness        0.596534\n",
      "mean concavity          0.696360\n",
      "mean concave points     0.776614\n",
      "radius error            0.567134\n",
      "perimeter error         0.556141\n",
      "area error              0.548236\n",
      "worst radius            0.776454\n",
      "worst perimeter         0.782914\n",
      "worst area              0.733825\n",
      "worst compactness       0.590998\n",
      "worst concavity         0.659610\n",
      "worst concave points    0.793566\n",
      "Name: target, dtype: float64\n",
      "['mean radius', 'mean perimeter', 'mean area', 'mean compactness', 'mean concavity', 'mean concave points', 'radius error', 'perimeter error', 'area error', 'worst radius', 'worst perimeter', 'worst area', 'worst compactness', 'worst concavity', 'worst concave points']\n"
     ]
    }
   ],
   "source": [
    "# 2- Report the features that their correlation is more than 0.5                #\n",
    "\n",
    "big_corr = corr[corr > 0.5]\n",
    "print(\"* Features with correlation bigger that 0.5 : \\n\", big_corr)\n",
    "features2 = big_corr.index.tolist()\n",
    "print(features2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 3 features with least inter-correlation with other features :\n",
      "['worst compactness', 'worst concavity', 'area error']\n"
     ]
    }
   ],
   "source": [
    "# 3- compute the correlation between the features you reported in 2nd           #\n",
    "# section and report features that their correlation with other features        #\n",
    "# is less than 0.5                                                              #\n",
    "\n",
    "inner_corr = np.abs(df[big_corr.index.tolist()].corr()[big_corr.index.tolist()][:])\n",
    "print(\"* 3 features with least inter-correlation with other features :\")\n",
    "features3 = ((inner_corr < 0.5) * 1).sum().sort_values(ascending=False)[0:3].index.tolist()\n",
    "print(features3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean perimeter             8.990911\n",
      "worst perimeter            8.292414\n",
      "mean area                  4.626827\n",
      "worst texture              2.760780\n",
      "mean texture               2.500252\n",
      "worst radius               1.585300\n",
      "mean radius                1.576180\n",
      "texture error              0.225106\n",
      "worst symmetry             0.034968\n",
      "mean symmetry              0.030117\n",
      "worst smoothness           0.018330\n",
      "radius error               0.016309\n",
      "mean smoothness            0.015808\n",
      "mean fractal dimension     0.012981\n",
      "worst fractal dimension    0.009545\n",
      "perimeter error            0.006179\n",
      "symmetry error             0.004007\n",
      "smoothness error           0.001658\n",
      "fractal dimension error    0.000327\n",
      "None                       0.000000\n",
      "concave points error      -0.000227\n",
      "concavity error           -0.002090\n",
      "compactness error         -0.002239\n",
      "mean compactness          -0.010855\n",
      "mean concave points       -0.013424\n",
      "worst concave points      -0.022949\n",
      "mean concavity            -0.031360\n",
      "worst compactness         -0.058753\n",
      "worst concavity           -0.085331\n",
      "area error                -2.572037\n",
      "worst area                -5.612990\n",
      "dtype: float64\n",
      "The test data accuracy is:  90.35087719298247\n"
     ]
    }
   ],
   "source": [
    "# 4- use perceptron from sklearn package to classify the data. Report accurracy #\n",
    "# for test data and sort the features based on their weights in perceptron.     #\n",
    "# IMPORTANT: Don't forget to add 1s to the end of feature vectors to be         #\n",
    "# multiplied by bias term of weight in perceptron.                              #\n",
    "\n",
    "# Note: these two lines should be executed only once\n",
    "X_train = np.insert(X_train, 0, 1, axis=1)\n",
    "X_test = np.insert(X_test, 0, 1, axis=1)\n",
    "index = np.insert(corr.index.tolist(), 0, None)\n",
    "\n",
    "clf = Perceptron(tol=1e-3, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "# coef_ is weights assigned to features\n",
    "coeffs = clf.coef_[0]\n",
    "\n",
    "print(\"Sorted features list : \")\n",
    "print(pd.Series(data=coeffs, index=index).sort_values(ascending=False))\n",
    "val_preds = clf.predict(X_test)\n",
    "print('The test data accuracy is: ', (accuracy_score(y_test,val_preds) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5- compare the features you reported in section 2 and 3 with the features     #\n",
    "# that have the most weights in perceptron and write your analysis below        #\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 - Classify data with perceptron and use only the features you repoted in    # \n",
    "# section 2 and report accuracy for test data.                                  #\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 - Do the same with section 3 and compare accuracies.                        #\n",
    "\n",
    "\n",
    "\n",
    "#################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o0QJrI36-GNR"
   },
   "source": [
    "### explanation of part 5 and 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iTC2KZ1h-QO6"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k_XG0iKSRqnE"
   },
   "source": [
    "### Question: \n",
    "Is it important to extract features before classifying using methods like decision tree and SVM? why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ro6v3JSBRsd5"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eQjRm5a6j8KZ"
   },
   "source": [
    "## problem 5. mRMR (10 bonus points) </br>\n",
    "In this part you should write your own code and classify the data using mRMR method.You can use \"pymrmr\" package for this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pymrmr"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MLBio_HW3_P.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
