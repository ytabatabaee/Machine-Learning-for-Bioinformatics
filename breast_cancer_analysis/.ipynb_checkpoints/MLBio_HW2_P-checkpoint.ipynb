{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning in Bioinformatics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2: \n",
    " !!! If you don't fill these fields, your homework does not count !!!<by/>\n",
    " #### first name and last name : Yasamin Tabatabaee\n",
    " #### student number : 95104866"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run cells by hitting `Shift` + `Enter` or `ctrl` + `Enter`. <br/>\n",
    "We highly recommend you to read each line of code carefully and try to understand what it exactly does. <br/>\n",
    "Just alter the parts that is between green comments and specified for you. Please do not change other parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1. soft margin SVM\n",
    "### about the Data:<br/>\n",
    "The purpose of this project is to classify tumors into malignant or benign. The following dataset is constructed based on images of tumors. Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass.\n",
    "For more details about the features of this dataset you can visit this link:\n",
    "https://scikit-learn.org/stable/datasets/index.html#breast-cancer-dataset<br/>\n",
    "This dataset contains 30 features and 1 label that is called target. We should find a proper hyperplane that separates malignant and benign samples.\n",
    "The original dataset labels is 0 and 1 and in the following code boxes we change it to -1 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                 0.07871  ...          17.33           184.60      2019.0   \n",
       "1                 0.05667  ...          23.41           158.80      1956.0   \n",
       "2                 0.05999  ...          25.53           152.50      1709.0   \n",
       "3                 0.09744  ...          26.50            98.87       567.7   \n",
       "4                 0.05883  ...          16.67           152.20      1575.0   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  target  \n",
       "0          0.4601                  0.11890     0.0  \n",
       "1          0.2750                  0.08902     0.0  \n",
       "2          0.3613                  0.08758     0.0  \n",
       "3          0.6638                  0.17300     0.0  \n",
       "4          0.2364                  0.07678     0.0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer = load_breast_cancer()\n",
    "df = pd.DataFrame(np.c_[cancer[\"data\"], cancer[\"target\"]], columns = np.append(cancer[\"feature_names\"],[\"target\"]))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.8804920913884 %\n",
      "71.8804920913884 %\n",
      "20.035149384885763 %\n",
      "20.035149384885763 %\n",
      "8.084358523725834 %\n",
      "8.084358523725834 %\n"
     ]
    }
   ],
   "source": [
    "cancer.target = np.where(cancer.target==0, -1, cancer.target) \n",
    "X_train ,X_test ,X_val ,y_train ,y_test ,y_val = None ,None ,None ,None ,None ,None\n",
    "################################################################################\n",
    "# TODO: using train_test_split package, split your data into 3 numpy array     #\n",
    "# called X_train, X_test, and X_val and also split the corresponding labels as #\n",
    "# y_train, y_test, and y_val. After spliting, the ratio of your data should be # \n",
    "# approximately like this:                                                     #\n",
    "#  Train : 72%     test : 20%       validation : 8%                            #\n",
    "################################################################################\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size = 0.2)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.1)\n",
    "\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             #\n",
    "################################################################################\n",
    "print((X_train.shape[0]/cancer.data.shape[0]) * 100, \"%\")\n",
    "print((y_train.shape[0]/cancer.data.shape[0]) * 100, \"%\")\n",
    "print((X_test.shape[0]/cancer.data.shape[0]) * 100, \"%\")\n",
    "print((y_test.shape[0]/cancer.data.shape[0]) * 100, \"%\")\n",
    "print((X_val.shape[0]/cancer.data.shape[0]) * 100, \"%\")\n",
    "print((y_val.shape[0]/cancer.data.shape[0]) * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### soft margin SVM optimization:<br/>\n",
    "We add 1 at the beginning of each Xs data (X_train, X_val , ...) and then the bias will be calculated implicitly.\n",
    "Then you should minimize the following SVM loss function (using gradient descent) with changing parameters of model.<br>\n",
    "In this notation: \n",
    "\\begin{equation}\n",
    "x_i , y_i\n",
    "\\end{equation}\n",
    "refers to feature vector of the sample and the label of our training data<br>\n",
    "and this is SVM loss function:\n",
    "\n",
    "\\begin{equation}\n",
    "\\large\n",
    "J(W) = \\frac{1}{N} \\sum_{i=1}^{N}{L^{(i)}} + \\frac{\\lambda}{2} ||W||^2\\\\\n",
    "\\large\n",
    "L^{(i)} ={max(0, 1 - y_i(w^{T}x_i)})\n",
    "\\;\\\\\n",
    "\\end{equation} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(409, 31)\n",
      "(46, 31)\n",
      "(114, 31)\n"
     ]
    }
   ],
   "source": [
    "# >>>>>WARNING: RUN THIS CELL ONLY ONCE!<<<<<\n",
    "\n",
    "# adding 1s to the end of feature vectors to be multiplied by bias term of weights\n",
    "X_val = np.insert(X_val, 0, 1, axis=1)\n",
    "X_train = np.insert(X_train, 0, 1, axis=1)\n",
    "X_test = np.insert(X_test, 0, 1, axis=1)\n",
    "print(X_train.shape)  \n",
    "print(X_val.shape)  \n",
    "print(X_test.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following functions in SVM class. In the part that you should compute loss function of this class, you are not allowed to use \"for\" loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-12T16:30:29.561420Z",
     "start_time": "2020-03-12T16:30:29.538696Z"
    }
   },
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    def __init__(self, n_features: int, std: float):\n",
    "        \"\"\"\n",
    "        n_features: number of features in (or the dimension of) each instance\n",
    "        std: standard deviation used in the initialization of the weights of svm\n",
    "        \"\"\"\n",
    "        self.n_features = n_features\n",
    "        ################################################################################\n",
    "        # TODO: Initialize the weights of svm using random normal distribution with    #\n",
    "        # standard deviation equals to std.                                            #\n",
    "        ################################################################################\n",
    "\n",
    "        self.w = np.random.normal(scale=std, size=n_features)\n",
    "\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             #\n",
    "        ################################################################################\n",
    "\n",
    "    def loss(self, X: np.ndarray, y: np.ndarray, reg_coeff: float):\n",
    "        \"\"\"\n",
    "        X: training instances as a 2d-array with shape (num_train, n_features)\n",
    "        y: labels corresponsing to the given training instances as a 1d-array with shape (num_train,)\n",
    "        reg_coeff: L2-regularization coefficient\n",
    "        \"\"\"\n",
    "        loss = 0.0\n",
    "        \n",
    "        #################################################################################\n",
    "        # TODO: Compute the hinge loss specified in the notebook and save it in the loss#                                                   # loss variable.                                                               #\n",
    "        # NOTE: YOU ARE NOT ALLOWED TO USE FOR LOOPS!                                   #\n",
    "        # Don't forget L2-regularization term in your implementation!                   #\n",
    "        #################################################################################\n",
    "        \n",
    "        hinge_loss = np.maximum(0, 1 - y * np.dot(X, self.w))\n",
    "        loss = np.sum(hinge_loss) / self.n_features + reg_coeff / 2 * np.linalg.norm(self.w) ** 2\n",
    "\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             #\n",
    "        ################################################################################\n",
    "        return loss\n",
    "        \n",
    "    def update_weights(self,  X: np.ndarray, y: np.ndarray, learning_rate: float , reg_coeff: float):\n",
    "        \"\"\"\n",
    "        Updates the weights of the svm using the gradient of computed loss with respect to the weights. \n",
    "        learning_rate: learning rate that will be used in gradient descent to update the weights\n",
    "        \"\"\"\n",
    "        ################################################################################\n",
    "        # TODO: Compute the gradient of loss computed above w.r.t the svm weights.     #\n",
    "        # and then update self.w with the computed gradient.                           #\n",
    "        # (don't forget learning rate and reg_coeff in update rule)                    #\n",
    "        # Don't forget L2-regularization term in your implementation!                  #\n",
    "        ################################################################################\n",
    "        \n",
    "        loss = np.maximum(0, 1 - y * np.dot(X, self.w))\n",
    "        loss_indicator = np.zeros(loss.shape)\n",
    "        loss_indicator[loss > 0] = 1\n",
    "        gradient = 0.0\n",
    "        for i in range(self.n_features):\n",
    "            gradient += reg_coeff * self.w - loss_indicator[i] * X[i] * y[i]    \n",
    "        self.w = self.w - learning_rate * gradient / self.n_features\n",
    "\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             #\n",
    "        ################################################################################\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        X: Numpy 2d-array of instances\n",
    "        \"\"\"\n",
    "        y_pred = None\n",
    "        ################################################################################\n",
    "        # TODO: predict the labels for the instances in X and save them in y_pred.     #                                      #\n",
    "        ################################################################################\n",
    "\n",
    "        y_pred = np.sign(np.dot(X, self.w))\n",
    "        \n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             #\n",
    "        ################################################################################\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell contains your hyper parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = 0.0001\n",
    "num_iters = 15000\n",
    "reg_coeff = 20\n",
    "learning_rate=1e-8\n",
    "np.random.seed(123456)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell using your SVM class, we want to train our model for cancer data:<br/>\n",
    "In every iteration you should see your training loss decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, loss 13.118367, val acc 69.57%\n",
      "iteration 100, loss 12.903853, val acc 45.65%\n",
      "iteration 200, loss 12.689342, val acc 45.65%\n",
      "iteration 300, loss 12.490204, val acc 45.65%\n",
      "iteration 400, loss 12.337388, val acc 45.65%\n",
      "iteration 500, loss 12.257228, val acc 45.65%\n",
      "iteration 600, loss 12.242381, val acc 45.65%\n",
      "iteration 700, loss 12.226843, val acc 45.65%\n",
      "iteration 800, loss 12.214647, val acc 45.65%\n",
      "iteration 900, loss 12.196339, val acc 45.65%\n",
      "iteration 1000, loss 12.171275, val acc 45.65%\n",
      "iteration 1100, loss 12.146473, val acc 45.65%\n",
      "iteration 1200, loss 12.121996, val acc 45.65%\n",
      "iteration 1300, loss 12.098094, val acc 45.65%\n",
      "iteration 1400, loss 12.074511, val acc 45.65%\n",
      "iteration 1500, loss 12.051298, val acc 45.65%\n",
      "iteration 1600, loss 12.028086, val acc 45.65%\n",
      "iteration 1700, loss 12.005086, val acc 45.65%\n",
      "iteration 1800, loss 11.982153, val acc 45.65%\n",
      "iteration 1900, loss 11.961025, val acc 45.65%\n",
      "iteration 2000, loss 11.941589, val acc 45.65%\n",
      "iteration 2100, loss 11.922479, val acc 45.65%\n",
      "iteration 2200, loss 11.903044, val acc 45.65%\n",
      "iteration 2300, loss 11.883944, val acc 45.65%\n",
      "iteration 2400, loss 11.864500, val acc 45.65%\n",
      "iteration 2500, loss 11.845067, val acc 45.65%\n",
      "iteration 2600, loss 11.825960, val acc 45.65%\n",
      "iteration 2700, loss 11.806525, val acc 45.65%\n",
      "iteration 2800, loss 11.787562, val acc 45.65%\n",
      "iteration 2900, loss 11.768353, val acc 45.65%\n",
      "iteration 3000, loss 11.749160, val acc 45.65%\n",
      "iteration 3100, loss 11.730233, val acc 45.65%\n",
      "iteration 3200, loss 11.711034, val acc 45.65%\n",
      "iteration 3300, loss 11.692283, val acc 45.65%\n",
      "iteration 3400, loss 11.673793, val acc 45.65%\n",
      "iteration 3500, loss 11.655124, val acc 45.65%\n",
      "iteration 3600, loss 11.636644, val acc 45.65%\n",
      "iteration 3700, loss 11.617999, val acc 45.65%\n",
      "iteration 3800, loss 11.599366, val acc 45.65%\n",
      "iteration 3900, loss 11.581025, val acc 45.65%\n",
      "iteration 4000, loss 11.562420, val acc 45.65%\n",
      "iteration 4100, loss 11.544106, val acc 45.65%\n",
      "iteration 4200, loss 11.525489, val acc 45.65%\n",
      "iteration 4300, loss 11.506886, val acc 45.65%\n",
      "iteration 4400, loss 11.488565, val acc 45.65%\n",
      "iteration 4500, loss 11.469958, val acc 45.65%\n",
      "iteration 4600, loss 11.451356, val acc 45.65%\n",
      "iteration 4700, loss 11.433032, val acc 45.65%\n",
      "iteration 4800, loss 11.414430, val acc 45.65%\n",
      "iteration 4900, loss 11.396115, val acc 45.65%\n",
      "iteration 5000, loss 11.377506, val acc 45.65%\n",
      "iteration 5100, loss 11.358906, val acc 45.65%\n",
      "iteration 5200, loss 11.340584, val acc 45.65%\n",
      "iteration 5300, loss 11.321985, val acc 45.65%\n",
      "iteration 5400, loss 11.303675, val acc 45.65%\n",
      "iteration 5500, loss 11.285064, val acc 45.65%\n",
      "iteration 5600, loss 11.266466, val acc 45.65%\n",
      "iteration 5700, loss 11.248150, val acc 45.65%\n",
      "iteration 5800, loss 11.229549, val acc 45.65%\n",
      "iteration 5900, loss 11.211245, val acc 45.65%\n",
      "iteration 6000, loss 11.192633, val acc 45.65%\n",
      "iteration 6100, loss 11.174037, val acc 45.65%\n",
      "iteration 6200, loss 11.155725, val acc 45.65%\n",
      "iteration 6300, loss 11.137123, val acc 45.65%\n",
      "iteration 6400, loss 11.118528, val acc 45.65%\n",
      "iteration 6500, loss 11.100211, val acc 45.65%\n",
      "iteration 6600, loss 11.081648, val acc 45.65%\n",
      "iteration 6700, loss 11.063434, val acc 45.65%\n",
      "iteration 6800, loss 11.044876, val acc 45.65%\n",
      "iteration 6900, loss 11.026330, val acc 45.65%\n",
      "iteration 7000, loss 11.008108, val acc 45.65%\n",
      "iteration 7100, loss 10.989560, val acc 45.65%\n",
      "iteration 7200, loss 10.971351, val acc 47.83%\n",
      "iteration 7300, loss 10.952792, val acc 47.83%\n",
      "iteration 7400, loss 10.934247, val acc 47.83%\n",
      "iteration 7500, loss 10.916031, val acc 47.83%\n",
      "iteration 7600, loss 10.897482, val acc 47.83%\n",
      "iteration 7700, loss 10.878956, val acc 47.83%\n",
      "iteration 7800, loss 10.860843, val acc 47.83%\n",
      "iteration 7900, loss 10.842369, val acc 47.83%\n",
      "iteration 8000, loss 10.824267, val acc 47.83%\n",
      "iteration 8100, loss 10.805784, val acc 47.83%\n",
      "iteration 8200, loss 10.787311, val acc 47.83%\n",
      "iteration 8300, loss 10.769202, val acc 47.83%\n",
      "iteration 8400, loss 10.750728, val acc 50.00%\n",
      "iteration 8500, loss 10.732722, val acc 50.00%\n",
      "iteration 8600, loss 10.714401, val acc 52.17%\n",
      "iteration 8700, loss 10.696093, val acc 52.17%\n",
      "iteration 8800, loss 10.678229, val acc 54.35%\n",
      "iteration 8900, loss 10.659917, val acc 54.35%\n",
      "iteration 9000, loss 10.641611, val acc 54.35%\n",
      "iteration 9100, loss 10.623753, val acc 54.35%\n",
      "iteration 9200, loss 10.605462, val acc 54.35%\n",
      "iteration 9300, loss 10.587664, val acc 54.35%\n",
      "iteration 9400, loss 10.569365, val acc 54.35%\n",
      "iteration 9500, loss 10.551075, val acc 56.52%\n",
      "iteration 9600, loss 10.533270, val acc 56.52%\n",
      "iteration 9700, loss 10.514980, val acc 56.52%\n",
      "iteration 9800, loss 10.497187, val acc 56.52%\n",
      "iteration 9900, loss 10.478886, val acc 56.52%\n",
      "iteration 10000, loss 10.460598, val acc 58.70%\n",
      "iteration 10100, loss 10.442799, val acc 56.52%\n",
      "iteration 10200, loss 10.424507, val acc 60.87%\n",
      "iteration 10300, loss 10.406221, val acc 63.04%\n",
      "iteration 10400, loss 10.388477, val acc 63.04%\n",
      "iteration 10500, loss 10.370297, val acc 63.04%\n",
      "iteration 10600, loss 10.352688, val acc 63.04%\n",
      "iteration 10700, loss 10.334501, val acc 63.04%\n",
      "iteration 10800, loss 10.316322, val acc 65.22%\n",
      "iteration 10900, loss 10.298706, val acc 67.39%\n",
      "iteration 11000, loss 10.280528, val acc 67.39%\n",
      "iteration 11100, loss 10.262925, val acc 67.39%\n",
      "iteration 11200, loss 10.244736, val acc 69.57%\n",
      "iteration 11300, loss 10.226559, val acc 71.74%\n",
      "iteration 11400, loss 10.208949, val acc 71.74%\n",
      "iteration 11500, loss 10.190770, val acc 71.74%\n",
      "iteration 11600, loss 10.173172, val acc 71.74%\n",
      "iteration 11700, loss 10.154982, val acc 71.74%\n",
      "iteration 11800, loss 10.136806, val acc 73.91%\n",
      "iteration 11900, loss 10.119202, val acc 73.91%\n",
      "iteration 12000, loss 10.101021, val acc 73.91%\n",
      "iteration 12100, loss 10.082847, val acc 73.91%\n",
      "iteration 12200, loss 10.065236, val acc 73.91%\n",
      "iteration 12300, loss 10.047063, val acc 73.91%\n",
      "iteration 12400, loss 10.029509, val acc 73.91%\n",
      "iteration 12500, loss 10.011395, val acc 76.09%\n",
      "iteration 12600, loss 9.993293, val acc 76.09%\n",
      "iteration 12700, loss 9.975790, val acc 76.09%\n",
      "iteration 12800, loss 9.957686, val acc 73.91%\n",
      "iteration 12900, loss 9.940196, val acc 73.91%\n",
      "iteration 13000, loss 9.922080, val acc 78.26%\n",
      "iteration 13100, loss 9.903980, val acc 78.26%\n",
      "iteration 13200, loss 9.886482, val acc 78.26%\n",
      "iteration 13300, loss 9.868377, val acc 78.26%\n",
      "iteration 13400, loss 9.850277, val acc 78.26%\n",
      "iteration 13500, loss 9.832775, val acc 78.26%\n",
      "iteration 13600, loss 9.814676, val acc 80.43%\n",
      "iteration 13700, loss 9.797185, val acc 80.43%\n",
      "iteration 13800, loss 9.779077, val acc 80.43%\n",
      "iteration 13900, loss 9.760979, val acc 80.43%\n",
      "iteration 14000, loss 9.743530, val acc 80.43%\n",
      "iteration 14100, loss 9.725491, val acc 82.61%\n",
      "iteration 14200, loss 9.708098, val acc 82.61%\n",
      "iteration 14300, loss 9.690047, val acc 82.61%\n",
      "iteration 14400, loss 9.672009, val acc 84.78%\n",
      "iteration 14500, loss 9.654609, val acc 84.78%\n",
      "iteration 14600, loss 9.636568, val acc 84.78%\n",
      "iteration 14700, loss 9.618605, val acc 84.78%\n",
      "iteration 14800, loss 9.601331, val acc 84.78%\n",
      "iteration 14900, loss 9.583402, val acc 84.78%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "model = SVM(n_features=X_train.shape[1], std= std )\n",
    "loss_history = []\n",
    "loss_val_history = []\n",
    "for it in range(num_iters):\n",
    "    loss = model.loss(X_train, y_train, reg_coeff)\n",
    "    loss_val = model.loss(X_val, y_val, reg_coeff)\n",
    "    if it % 100 == 0:\n",
    "        val_preds =  model.predict(X_val)\n",
    "        print('iteration %d, loss %f, val acc %.2f%%' % (it, loss,  accuracy_score(y_val,val_preds) * 100))\n",
    "    model.update_weights(X_train, y_train, learning_rate , reg_coeff)\n",
    "    loss_history.append(loss)\n",
    "    loss_val_history.append(loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "####################################################################################\n",
    "# TODO: using matplotlib.pyplot package plot the training loss and validation loss #\n",
    "# using loss_loss_history and loss_val_history                                     #\n",
    "####################################################################################\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2)\n",
    "fig.suptitle('Loss Changes')\n",
    "ax1.set(xlabel='Iteration', ylabel='Loss', xticks = np.arange(0, num_iters + 1, 1000))\n",
    "ax2.set(xlabel='Iteration', ylabel='Loss', xticks = np.arange(0, num_iters + 1, 1000))\n",
    "ax1.scatter(range(0, num_iters), loss_history, label='Training')\n",
    "ax2.scatter(range(0, num_iters), loss_val_history, label='Validation')\n",
    "ax1.legend(loc='upper right')\n",
    "ax2.legend(loc='upper right')\n",
    "plt.show()    \n",
    "\n",
    "####################################################################################\n",
    "#                                 END OF YOUR CODE                                 #\n",
    "####################################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1:\n",
    "With changing your hyper parameters, find a configuration of hyper parameters that cause your loss to increase after each iteration and then report that configuration in the next cell. Explain why our loss increases?\n",
    "Write your answer in \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "std = \"\" <br>\n",
    "num_iters = \"\"<br>\n",
    "reg_coeff = \"\"<br>\n",
    "learning_rate = \"\"<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2:\n",
    "In this cell please explain the reason of this event<br>\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds =  model.predict(X_test)\n",
    "###########################################################################################\n",
    "# TODO: find the Confusion Matrix between val_preds and real labels (y_test) for test data#\n",
    "# then report the accuracy of the model.                                                  #\n",
    "# you are not allowed to use any premade function for accuracy and confusion matrix       #\n",
    "###########################################################################################\n",
    "\n",
    "#write your code here\n",
    "\n",
    "###########################################################################################\n",
    "#                                END OF YOUR EXPLANATION                                  #\n",
    "###########################################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next part, we sample from training data with certain size (batch size) instead of using all the training data in each iteration, and train our model on batch data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = 0.0001\n",
    "batch_size = 200\n",
    "num_iters = 15000\n",
    "reg_coeff = 20\n",
    "learning_rate=1e-8\n",
    "model = SVM(n_features=X_train.shape[1], std= std )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'NoneType' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-d489691bb8f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m#                                 END OF YOUR CODE                             #\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m################################################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg_coeff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mloss_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg_coeff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mit\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-35271e3527e5>\u001b[0m in \u001b[0;36mloss\u001b[1;34m(self, X, y, reg_coeff)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m#################################################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mhinge_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhinge_loss\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_features\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mreg_coeff\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'NoneType' and 'float'"
     ]
    }
   ],
   "source": [
    "loss_history = []\n",
    "loss_val_history = []\n",
    "for it in range(num_iters):\n",
    "    X_batch = None\n",
    "    y_batch = None\n",
    "    ################################################################################\n",
    "    # TODO: Sample batch_size elements from the training data and their            #\n",
    "    # corresponding labels to use in this round of gradient descent.               #\n",
    "    # Store the data in X_batch and their corresponding labels in                  #\n",
    "    # y_batch; after sampling X_batch should have shape (batch_size, n_features)   #\n",
    "    # and y_batch should have shape (batch_size,)                                  #\n",
    "    #                                                                              #\n",
    "    # Hint: Use np.random.choice to generate indices. Sampling with                #\n",
    "    # replacement is faster than sampling without replacement.                     #\n",
    "    ################################################################################\n",
    "\n",
    "    #write your code here\n",
    "\n",
    "    ################################################################################\n",
    "    #                                 END OF YOUR CODE                             #\n",
    "    ################################################################################\n",
    "    loss = model.loss(X_batch, y_batch, reg_coeff)\n",
    "    loss_val = model.loss(X_val, y_val, reg_coeff)\n",
    "    if it % 100 == 0:\n",
    "        val_preds =  model.predict(X_val)\n",
    "        print('iteration %d, loss %f, val acc %.2f%%' % (it, loss,  accuracy_score(y_val,val_preds) * 100))\n",
    "    model.update_weights(X_batch, y_batch, learning_rate , reg_coeff)\n",
    "    loss_history.append(loss)\n",
    "    loss_val_history.append(loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "# TODO: using matplotlib.pyplot package plot the training loss and validation loss #\n",
    "\n",
    "####################################################################################\n",
    "\n",
    "#write your code here\n",
    "\n",
    "####################################################################################\n",
    "#                                 END OF YOUR CODE                                 #\n",
    "####################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3:\n",
    "Explain why we see fluctuation in this plot?<br>\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2. Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now, you should know how to use and how to implement SVM from scratch.\n",
    "In fact, for perceptron we can use premade functions as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "clf = Perceptron(tol=1e-3, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "val_preds = clf.predict(X_test)\n",
    "print(confusion_matrix(y_test,val_preds,[1,-1]))\n",
    "print('test acc %.2f%%' % (accuracy_score(y_test,val_preds) * 100)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4:\n",
    "It is time for you to write your own code instead of completing some parts of a premade code.<br>\n",
    "So you can not use any premade functions like the previous cell.\n",
    "Write your code in the end of this .ipynb file <br>\n",
    "You should make your model and use that to build these outputs:<br>\n",
    "1- Report loss of training and accuracy of validation data on each epoch of training process.<br>\n",
    "2- You are allowed to use any normalization approach if need be.<br>\n",
    "3- Plot your training and validation loss vs number of iterations in one plot.<br>\n",
    "4- Finally print your confusion matrix and accuracy for your testing set.<br>\n",
    "With changing your hyperparameters try to get a good and reasonable accuracy and confusion matrix on testing set (similar to the accuracy when we used the package)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
